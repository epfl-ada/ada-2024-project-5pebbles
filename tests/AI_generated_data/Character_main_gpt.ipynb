{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ed10ac-3d37-4c09-973e-0e2ebb9876b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T18:09:38.510703Z",
     "iopub.status.busy": "2024-11-29T18:09:38.509377Z",
     "iopub.status.idle": "2024-11-29T18:09:52.573204Z",
     "shell.execute_reply": "2024-11-29T18:09:52.569339Z",
     "shell.execute_reply.started": "2024-11-29T18:09:38.510544Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 33080/103071 [00:03<00:10, 6874.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Margarita Terekhova in nan: sequence item 1: expected str instance, float found\n",
      "Error processing nan in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Valentin Smirnitsky in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Irina Alfyorova in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Lev Durov in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Veniamin Smekhov in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Alisa Freindlich in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Igor Starygin in nan: sequence item 1: expected str instance, float found\n",
      "Error processing Oleg Tabakov in nan: sequence item 1: expected str instance, float found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 78274/103071 [00:11<00:03, 6525.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed!\n",
      "Total characters processed: 103071\n",
      "Characters with GPT decision: 78265\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class GPTAnalyzer:\n",
    "    def __init__(self, api_keys_path='MovieSummaries/api_keys.json'):\n",
    "        with open(api_keys_path, 'r') as f:\n",
    "            api_keys = json.load(f)\n",
    "            \n",
    "        self.gpt4_model = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            api_key=api_keys['openai_key'],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.temp_file = 'MovieSummaries/temp_gpt_results.tsv'\n",
    "        self.final_file = 'MovieSummaries/character_metadata_with_gpt.tsv'\n",
    "        self.plot_file = 'MovieSummaries/plot_summaries.txt'\n",
    "        self.plot_summaries = self._load_plot_summaries()\n",
    "    \n",
    "    def _load_plot_summaries(self):\n",
    "        summaries = {}\n",
    "        with open(self.plot_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                wiki_id, summary = line.strip().split('\\t', 1)\n",
    "                summaries[wiki_id] = summary\n",
    "        return summaries\n",
    "    \n",
    "    def _load_existing_results(self):\n",
    "        if os.path.exists(self.temp_file):\n",
    "            return pd.read_csv(self.temp_file, sep='\\t')\n",
    "        return None\n",
    "    \n",
    "    def _create_prompt(self, movie_name, plot_summary, all_actors, target_actor):\n",
    "        prompt = f\"\"\"\n",
    "        Movie: {movie_name}\n",
    "        Plot Summary: {plot_summary}\n",
    "        All actors in the movie: {', '.join(all_actors)}\n",
    "        \n",
    "        Is the actor \"{target_actor}\" a main character in this movie?\n",
    "        Please only answer with \"True\" if they are a main character, or \"False\" if they are not.\n",
    "        ANSWER WITH ONLY TRUE OR FALSE.\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def extract_decision(self, response):\n",
    "        response = response.lower()\n",
    "        has_true = 'true' in response\n",
    "        has_false = 'false' in response\n",
    "        \n",
    "        if (has_true and has_false) or (not has_true and not has_false):\n",
    "            return None\n",
    "        return has_true\n",
    "    \n",
    "    async def process_single_character(self, row, movie_name, plot_summary, all_actors):\n",
    "        try:\n",
    "            prompt = self._create_prompt(movie_name, plot_summary, all_actors, row['Actor name'])\n",
    "            gpt_response = await asyncio.to_thread(self.gpt4_model.predict, prompt)\n",
    "            gpt_response = gpt_response.strip().lower()\n",
    "            gpt_decision = self.extract_decision(gpt_response)\n",
    "            return row.name, gpt_decision\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['Actor name']} in {movie_name}: {str(e)}\")\n",
    "            return row.name, None\n",
    "\n",
    "    async def process_movie_batch(self, movie_group, wiki_id_str, batch_semaphore):\n",
    "        async with batch_semaphore:\n",
    "            movie_name = movie_group['Movie name'].iloc[0]\n",
    "            plot_summary = self.plot_summaries[wiki_id_str]\n",
    "            all_actors = movie_group['Actor name'].tolist()\n",
    "            \n",
    "            tasks = []\n",
    "            for _, row in movie_group.iterrows():\n",
    "                if pd.isna(row['gpt_decision']):\n",
    "                    tasks.append(self.process_single_character(row, movie_name, plot_summary, all_actors))\n",
    "            \n",
    "            if tasks:\n",
    "                return await asyncio.gather(*tasks)\n",
    "            return []\n",
    "\n",
    "    async def analyze_characters_async(self, character_data, max_concurrent=32, batch_size=100):\n",
    "        existing_results = self._load_existing_results()\n",
    "        if existing_results is not None:\n",
    "            character_data = pd.concat([\n",
    "                character_data,\n",
    "                existing_results[['Wikipedia movie ID', 'Actor name', 'gpt_decision']]\n",
    "            ]).drop_duplicates(subset=['Wikipedia movie ID', 'Actor name'], keep='last')\n",
    "        \n",
    "        if 'gpt_decision' not in character_data.columns:\n",
    "            character_data['gpt_decision'] = None\n",
    "        \n",
    "        movie_groups = character_data.groupby('Wikipedia movie ID')\n",
    "        batch_semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        progress_bar = tqdm(total=len(character_data))\n",
    "        count = 0\n",
    "        \n",
    "        for wiki_id, group in movie_groups:\n",
    "            wiki_id_str = str(wiki_id)\n",
    "            if wiki_id_str not in self.plot_summaries:\n",
    "                continue\n",
    "            \n",
    "            results = await self.process_movie_batch(group, wiki_id_str, batch_semaphore)\n",
    "            \n",
    "            for idx, decision in results:\n",
    "                character_data.loc[idx, 'gpt_decision'] = decision\n",
    "                count += 1\n",
    "                \n",
    "                if count % batch_size == 0:\n",
    "                    character_data.to_csv(self.temp_file, sep='\\t', index=False)\n",
    "            \n",
    "            progress_bar.update(len(group))\n",
    "        \n",
    "        progress_bar.close()\n",
    "        character_data.to_csv(self.final_file, sep='\\t', index=False)\n",
    "        return character_data\n",
    "\n",
    "def main():\n",
    "    character_data = pd.read_csv('MovieSummaries/character_metadata_with_movies.tsv', sep='\\t')\n",
    "    analyzer = GPTAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    \n",
    "    result_data = loop.run_until_complete(analyzer.analyze_characters_async(character_data))\n",
    "    \n",
    "    print(\"Analysis completed!\")\n",
    "    print(f\"Total characters processed: {len(result_data)}\")\n",
    "    print(f\"Characters with GPT decision: {result_data['gpt_decision'].notna().sum()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cbb0a-f1b8-48b6-a55c-4d5591080cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
